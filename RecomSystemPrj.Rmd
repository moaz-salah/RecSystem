---
title: "Movies Recommendation Project"
author: "Moaz M. Salaheldeen"
date: "2023-11-19"
output: pdf_document
---


# Introduction  

## Data Set

This dataset lists movies ratings from MovieLens. Dataset is available to download in [**grouplens**](https://files.grouplens.org/datasets/movielens/ml-10m.zip) site for public. 

The aim of this project is to explore this dataset and understand the affect of Users, Movies and other factors on the ratings.Then we will work on building a model that participate users' ratings for movies they didnt rate before.

To do so, we will do these steps:  

* Data Importing, cleansing and Preparation
* Data Exploring and Visualization
* Modeling
* Results and Conclusions


# Step 1: Data  Importing, Cleansing and Preparation    
```{r definitionChunk,  include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")   #require returns FALSE and gives a warning if the package does not exist
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

getwd()

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <-"ml-10M100K.zip"

if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)


ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE), stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")

ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE), stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")

movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")
```

```{r prepare_1_chunkChunk, include=FALSE}
#How many unique users/movies we have ? 
movielens|> summarize(n_users = n_distinct(userId))  # 69878
movielens|> summarize(n_movies = n_distinct(movieId))  # 10677
movielens|> summarize(min_rating = min(rating))  # .5
movielens|> summarize(max_rating = max(rating))  # 5
```
The data set we are using contains about 10 Millions ratings. These ratings were made by 69878 user for 10677 movies. If every user rated every movie, this would give more than 746 Million records, so we have less than 10% of the possible records that can be generated by these numbers of users and movies.

Every record consists of 6 columns: UserID as integer, MovieID as integer, rating (from .5 to 5) as numeric, Timestamp as integer, Title as character, and Genres as character. Genres can have more than one value separated by "|" separator.

```{r prepare_2_chunkChunk, echo=FALSE}
head(movielens)
```


## Data Preparation

```{r prepare_3_chunkChunk, include=FALSE}
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

final_holdout_test <- temp %>%
  semi_join(edx, by = "movieId") %>%         
  semi_join(edx, by = "userId")

removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

set.seed (1, sample.kind = "Rounding")

test_index2 <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
temp <- edx[test_index2,]
work_train_data <- edx[-test_index2,]

#MMS: Cleansing Data Again: make sure no movie in Test data has no match in Training data
work_test_data <- temp %>%
  semi_join(work_train_data, by = "movieId") %>%         
  semi_join(work_train_data, by = "userId")
work_train_data <- rbind(work_train_data,    anti_join(temp, work_test_data))
rm(temp,  test_index2)


##MMS: 2- Prepare Evaluation Function. Used Method is RMSE
Eval_RMSE <- function(true_ratings, predicted_ratings){    sqrt(mean((true_ratings - predicted_ratings)^2))}

```

10% of the dataset will be kept in _final_holdout_test_ for the Evaluation of the final model. 20% of the remaining is considered to be the test set and the other 80% is the training set. While creating a testing set, we made sure there is no record in the test set unless it has reference in the training set by movie and user. This is due to a limitation that will be elaborated later that the models we are hoping to develop work only for existing users and movies.

## Evaluation Criteria

The Evaluation function used to compare models is the standard RMSE: 
$RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big({d_i -f_i})^2}}$

## Reference Model

As a reference model, we will consider the average ratings for all movies,
```{r refMOdel_1_chunk,include=FALSE}
mu <- mean(work_train_data$rating, na.rm = TRUE)
Ref_rmse <- Eval_RMSE(work_test_data$rating, mu)
```

The Average in that case would be:
```{r  refMOdel_2_chunk,echo=FALSE}
mu     
```

We will create a table to keep the RMSE for every model we will create, so we can compare them. Now we have only one model, which is the base:

```{r  refModel_3_chunk,echo=FALSE}
rmse_results <- tibble(method = "Just the average", RMSE = Ref_rmse)
rmse_results
```

# Step 2: Data Visualization (& Insights)  

We will study some factors in more details:

_**Users**_  

Let's look at the number of movies rated by a single user. We will find the numbers ranges from 10 movies per user to 6616:
```{r userAnalysis_1_chunck, echo=FALSE}
head(movielens |> group_by(userId) %>%
       summarise(n=n()) %>%
       arrange(n))

head(movielens |> group_by(userId) %>%
       summarise(n=n()) %>%
       arrange(desc(n)))
```

If we look at the distribution of ratings, we will find that most of the users rate less than 100 movies
```{r userAnalysis_2_chunck, echo=FALSE}
movielens |> group_by(userId) %>%
  summarise(n=n()) %>%
  arrange(n) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "blue", fill="red") +
  scale_x_log10() + 
  ggtitle("Distribution of Users Rating") +
  xlab("Number of Ratings") +
  ylab("Number of Users") 
```



_**Movies**_  

Let's look at the number of rates movies get from both ends..the movies getting the smallest number of rates and movies getting rated the most
```{r MoviesAnalysis_1_chunck, echo=FALSE}

head(movielens |> group_by(movieId,title) %>%
       summarise(n=n()) %>%
       arrange(n))

head(movielens |> group_by(movieId,title) %>%
       summarise(n=n()) %>%
       arrange(desc(n)))
```
As can be seen, some movies are rated only one time, while others are rated in ten thousands. _Pulp Fiction_ and _Forrest Gump_ are the two movies getting most rated, with number of rates 31362 and 31079 respectively.

Looking at the distribution in the below diagram, we can see that most movies get rates around 50-500 
```{r MoviesAnalysis_2_chunck, echo=FALSE}

movielens |> group_by(movieId) %>%
  summarise(n=n()) %>%
  arrange(n) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "red", fill="blue") +
  scale_x_log10() + 
  ggtitle("Distribution of Movies Rating") +
  xlab("Number of Ratings") +
  ylab("Number of Movies") 
```


_**Ratings**_  

First, we check that there is no missing Rating, and we found none.
Then, we look at the rating distribution and find that 4 and 3 are the most given ratings by far than other ratings. 0.5 and 1.5 are the least given rates respectively.

```{r RatingAnalysis_1_chunck, echo=FALSE}
if(any( is.na(movielens[,"rating"])) )
  print("Missing Data") 

#What are the distribution of ratings?
movielens |> group_by(rating) %>%
  summarise(n=n()) %>%
  arrange(n) %>%
  ggplot(aes(x=rating,y=n, label=n)) +
  geom_text(hjust=0, vjust=0) +
  geom_line() +
  geom_point() +
  scale_x_log10() + 
  ggtitle("Distribution of Movies Rating") +
  xlab("Ratings") +
  ylab("Number of Ratings") 

```

_**Genres**_  
Genres field has multiple values separated by a "|". We need to check how many genres are attached to movies in general. Vast majority of movies get three or less Genres.

```{r GenresAnalysis_1_chunck, echo=FALSE}
tibble(count = str_count(movielens$genres, fixed("|")), genres = movielens$genres) %>% 
  group_by(count) %>%
  summarise(n = n()) %>%
  arrange(-count) %>% 
  ggplot(aes(x=count+1,y=n, label=n)) +
  geom_text(hjust=0, vjust=0) +
  geom_line() +
  geom_point() +
  ggtitle("Distribution of Genres per Movies") +
  xlab("Number of Genres") +
  ylab("Number of Movies") 

rm(movielens, edx)
```

Note that what we really count in genres is the separator. Therefore, if the count is 0, that means one Genre. 
```{r eval=FALSE}
str_count(movielens$genres, fixed("|")
```
That's why we added One to the X-Axis Values.


# Step 3: Modeling

## First Model: Linear


We will work to build a linear model based on the User biase and Movie biase. 
The model is $Y_u,_m=\mu+b_u+b_m+\epsilon_u,_m$

First, we look at the movie biase:   


**A.1 Add Movie Effect (Biase)**

Due to the size of data and the available computing power, we couldn't run the _lm()_ function, so we used an alternative method.
We converted the Training data into matrix, where columns are MoviesIDs, Rows are UserIDs ,and the Values are the Ratings given by a user for a movie.

So, to get the average biase for a movie, we get the average of the ratings given to that movie (that column) and subtract the general average from it.

From the below diagram, we can see that the highest biases ranges from -0.5 to 0.5, with more tend to negative biasesr  


```{r MovieBiase_1_chunck, include=FALSE}
y_mat <- select(work_train_data, movieId, userId, rating) |>
  pivot_wider(names_from = movieId, values_from = rating)       #Execution Time : 4-5 min
rnames <- y_mat$userId
y_mat <- as.matrix(y_mat[,-1])  #MMS: From table to Matrix
rownames(y_mat) <- rnames

b_m <- colMeans(y_mat - mu, na.rm = TRUE)            #MMS: Remove the Average 

fit_lm_m <- data.frame(movieId = as.integer(colnames(y_mat)), mu = mu, b_m = b_m)
Movie_rmse<- left_join(work_test_data, fit_lm_m , by = "movieId") |>   
  mutate(pred_m = mu + b_m) |> 
  summarize(M_rmse = Eval_RMSE(rating, pred_m))

rmse_results<- rmse_results %>% add_row(method = "Considering Movie Biase", RMSE = as.double(Movie_rmse))        # 0.944 
```

```{r MovieBiase_2_chunck, echo=FALSE}
fit_lm_m|> group_by(b_m)%>%
  summarise(n = n()) %>%
  ggplot(aes(x=b_m)) +
  geom_histogram(color = "red", fill="blue") +
  ggtitle("Distribution of Movies Biase") +
  xlab("Movie Biase") +
  ylab("Number of Movies") 
```

By now, we have created this part of the model: $Y_u,_m=\mu+b_m+\epsilon_u,_m$
If we compare this to the Ratings in Testing Dataset, and apply the Evaluation function, we can see enhancement over the reference model:

```{r MovieBiase_3_chunck, echo=FALSE}
rmse_results
```


**A.2 Add User Effect (Biase)**

We apply the same concept above, to get the average biase for a user; we get the average of the ratings given to that user (row) and subtract the general average AND the movie biase from it.

From the below diagram, we can see that User biase is normal distributed around 0  

```{r UserBiase_1_chunck}
gc()
#b_u <- rowMeans(sweep(y_mat - mu, 2, b_m), na.rm = TRUE)     #MMS: Remove the Average and the Movie Biase 

temp_Work_Data <- work_train_data |> group_by(movieId)  |>  summarize(b_m=mean(rating-mu))
work_train_data_Mod <- work_train_data|> left_join(temp_Work_Data, by='movieId')   |>   group_by(userId)  |>  summarize(b_u=mean(rating-mu-b_m))
b_u<- work_train_data_Mod$b_u

```
Note: In the above code I commented the statement used in the R file as it always gives _"Error: cannot allocate vector of size 5.6 Gb"_, and replaced with the code below it, that get the same result but consume less memory.  

```{r UserBiase_1b_chunck, include=FALSE}


fit_lm_m_u <- data.frame(userId = as.integer(rownames(y_mat)), b_u = b_u)

Movie_User_rmse<- left_join(work_test_data, fit_lm_m, by = "movieId") |> 
  left_join(fit_lm_m_u, by = "userId") |> 
  mutate(pred_m_u = mu + b_m + b_u) |> 
  summarize(M_U_rmse = Eval_RMSE(rating, pred_m_u))

rmse_results<- rmse_results %>% add_row(method = "Considering Movie & User Biase", RMSE = as.double(Movie_User_rmse))        # 0.8862152
```

```{r UserBiase_2_chunck, echo=FALSE}
fit_lm_m_u|> group_by(b_u)%>%
  summarise(n = n()) %>%
  ggplot(aes(x=b_u,)) +
  geom_histogram(color = "red", fill="blue") +
  ggtitle("Distribution of Users Biase") +
  xlab("Users Biase") +
  ylab("Number of Movies") 
```

By now, we have created this part of the model: $Y_u,_m=\mu+b_u+b_m+\epsilon_u,_m$
If we compare this to the Ratings in Testing Dataset, and apply the Evaluation function, we can see enhancement over the reference model:

```{r UserBiase_3_chunck, echo=FALSE}

rmse_results
```



**A.3 Enhance Movie Effect by considering Penalization Factor**
Before we finalize this model, we can do one enhancement to the Movie Biase. That is to consider the number of rates a movie get. So if a movie is rated 4 in average by 50 users for example, we can get more confidence on that rating compared to a movie with the same rate but rated by one user only.
Therefor, we will add $\lambda$ factor that penalize movies based on number of rates they receive, based on this equation: 
$b(\lambda) = \frac{1}{\lambda+n_m}\sum(Y-\mu)$

To select $\lambda$ value, we will use Cross Validation
```{r Reguliz_1_chunck, include=FALSE}
gc()
n <-  colSums(!is.na(y_mat))
lambdas <- seq(0, 10, 0.1)

sums <- colSums(y_mat - mu, na.rm = TRUE)
rmses <- sapply(lambdas, function(lambda){
  b_m <-  sums / (n + lambda)
  fit_lm_m$b_m <- b_m
  left_join(work_test_data, fit_lm_m, by = "movieId") |> mutate(pred = mu + b_m) |> 
    summarize(rmse = RMSE(rating, pred)) |>
    pull(rmse)
})
lambda <- lambdas[which.min(rmses)]

```

From this diagram, we can get the $\lambda$ value that minimize the RMSE 
```{r Reguliz_2_chunck, echo=FALSE}
qplot(lambdas, rmses)  
print(lambda)  #1.7
```

That will reduce RMSE as in this table
```{r Reguliz_13_chunck, include=FALSE}

fit_lm_m$b_m_reg <- colSums(y_mat - mu, na.rm = TRUE) / (n + lambda)  

RegMovie_User_rmse<- left_join(work_test_data, fit_lm_m, by = "movieId") |> 
  left_join(fit_lm_m_u, by = "userId") |> 
  mutate(pred_m_u_Reg = mu + b_m_reg + b_u) |> 
  summarize(M_U__Reg_rmse = Eval_RMSE(rating, pred_m_u_Reg))

rmse_results<- rmse_results %>% add_row(method = "Considering Regularized Movie & User Biase", RMSE = as.double(RegMovie_User_rmse))    
```

```{r Reguliz_4_chunck, echo=FALSE}
rmse_results
```


## Second Model: Matrix Factorization

We will use "recosystem" package that is R wrapper for recommender system using matrix factorization
First, we will need to put data in suitable format. This will be done using _data_memory_ function, as we are loading data from the r object used before. 
```{r matFact_1_chunck}
library(recosystem)
gc()

set.seed(1, sample.kind = "Rounding")

#MMS: Put data in needed frormat
train_data <-  with(work_train_data, data_memory(user_index = userId, 
                                                 item_index = movieId, 
                                                 rating     = rating))

reco_obj <-  recosystem::Reco()
```

Then, we will select the best tuning parameters using Cross Validation. We will leave most parameters to their default values, but will change these parameters:
_niter_ : to reduce it from default value of 20 to 5
_nthread_ : to increase it to allow for more parallel processing
The main reason to change these parameters is to speed the processing of this function, as it takes a lot of time. On my laptop, it takes around 20 minuets in average (with nthread  = 4, niter = 5)
```{r matFact_2_chunck}
opts <- reco_obj$tune(train_data, opts = list(dim = c(10, 20, 30), 
                                          #    lrate = c(0.1, 0.2),
                                          #    costp_l2 = c(0.01, 0.1), 
                                          #    costq_l2 = c(0.01, 0.1),
                                              nthread  = 4, niter = 10))          

```

After getting our tunned parameters, we will use them to train the model and try it on the Test Dataset, and turn the result back to normal Vector format in order to be able to run the Evaluation function.
```{r matFact_3_chunck, include=FALSE}
# Train the algorithm  
reco_obj$train(train_data, opts = c(opts$min, nthread = 4, niter = 10))   


test_data <-  with(work_test_data, data_memory(user_index = userId, 
                                               item_index = movieId, 
                                               rating     = rating))


y_hat_reco <-  reco_obj$predict(test_data, out_memory())

mf_rmse<- Eval_RMSE(work_test_data$rating, y_hat_reco)
rmse_results<- rmse_results %>% add_row(method = "Matrix Factorization", RMSE = as.double(mf_rmse))        # 0.7991247  
```

The results is: 

```{r matFact_4_chunck, echo=FALSE}
rmse_results
```


**Another Variation: Matrix Factorization on Residuals**
As a trial, I tried applying the same Matrix Factorization technique but on Residuals (After removing all known biases)  

```{r matFactRes_1_chunck}
# compute residuals on train set

work_train_data2 <- work_train_data %>% 
  left_join(fit_lm_m, by = "movieId") %>%
  left_join(fit_lm_m_u, by = "userId") %>%
  mutate(res = rating - mu - b_u - b_m_reg)

work_test_data2 <- work_test_data %>% 
  left_join(fit_lm_m, by = "movieId") %>%
  left_join(fit_lm_m_u, by = "userId") %>%
  mutate(res = rating - mu - b_u - b_m_reg)
```

And then applied the same tuning and training functions 

```{r matFactRes_2_chunck, include=FALSE}
gc()
train_data2 <-  with(work_train_data2, data_memory(user_index = userId, 
                                                  item_index = movieId, 
                                                  rating     = res,  index1 = T))

test_data2 <-  with(work_test_data2, data_memory(user_index = userId, 
                                                item_index = movieId, 
                                                rating     = res , index1 = T))
reco_obj2 <-  recosystem::Reco()

# Select the best tuning parameters
opts <- reco_obj2$tune(train_data, opts = list(dim = c(10, 20, 30), 
                                               nthread  = 4, niter = 5))   #MMS; about  22 min 

# Train the algorithm  
reco_obj2$train(train_data2, opts = c(opts$min, nthread = 4, niter = 10))   #Try 10;  
```

I added the Average $\mu$ to the resulted output before applying the Evaluation Function
```{r matFactRes_3_chunck}
y_hat_reco_2 <-  reco_obj2$predict(test_data2, out_memory())
y_hat_reco_2 <-  y_hat_reco_2 + mu
```

And this led to this result
```{r matFactRes_4_chunck, echo=FALSE}
mf_rmse_r<- Eval_RMSE(work_test_data2$rating, y_hat_reco_2)
rmse_results<- rmse_results %>% add_row(method = "Matrix Factorization Residual", RMSE = as.double(mf_rmse_r))    
rmse_results
```

**Final Model**
Although it is usually good practice to combine more than one algorithm together, but in that case, I'll work with one model, the best one we got -the one with the smallest RMSE- which is: **Matrix Factorization** model .  
This is only to avoid the limited computational power I have which led R to crash in sometimes!

One Final Step to do add to the model is to eliminate any value outside the predefined borders, so any predicted rating with value less than 0.5 will be adjusted to 0.5, and any predicted rating with value more than 5, will be corrected as 5

So, final model is:
```{r finalmodel, eval=FALSE}
y_hat_reco <-  reco_obj$predict(test_data, out_memory())
y_hat_reco[y_hat_reco<.5]<-.5
y_hat_reco[y_hat_reco>5]<-5
```



# Step 4: Results and Conclusions

## Test Model on Final Testing Data Set 

We start by transforming the _final_holdout_test_ to suitable format and then apply the model directly, followed by the simple data cleansing for out-of-borders points.
```{r finalEval, include=FALSE}
Validt_data <-  with(final_holdout_test, data_memory(user_index = userId, 
                                               item_index = movieId, 
                                               rating     = rating))

y_hat_reco_Final <-  reco_obj$predict(Validt_data, out_memory())

y_hat_reco_Final[y_hat_reco_Final<.5]<-.5
y_hat_reco_Final[y_hat_reco_Final>5]<-5

mf_rmse_Final<- Eval_RMSE(final_holdout_test$rating, y_hat_reco_Final)
```

Calculating the RMSE:
```{r finalEval_2, echo=FALSE}
mf_rmse_Final   

#MMS: Smallest and Largest difference  
final_holdout_test_predicted<-final_holdout_test |> mutate (predict= y_hat_reco_Final)

final_holdout_test_predicted |> arrange(desc(abs(rating-predict)))|> head(10)
final_holdout_test_predicted |> arrange(abs(rating-predict))|> head(10)
```


## Conclusion 
We tried different models. Linear model based on calculating the biases of Users and Movies. The other one was based on Matrix Factorization, and then tried Matrix Factorization on Residuals.

There are other algorithms that should be tested, like KNN model and Random Forest. Also, we didn't depend on Genres on any of these algorithms, despite the fact that it groups similar movies together and it is expected to really enhance the model. But again the size of the data combined with the limited CPU power limited this work on this phase.
One important limitation to note is that this algorithm cant predict a new user or a new movie. I expect if we include Genres (and maybe release date), we can overcome this limitation to some extent.  

Also, we shall dig more on the movies that got the largest difference between rating and prediction. We may figure out a pattern for these movies (Like low number of ratings or certain genre)  
 


